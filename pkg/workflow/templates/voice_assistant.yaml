name: voice_assistant
description: Voice input → ASR → LLM → TTS → Audio output

config:
  llm_model: "llama3.2"
  tts_model: "tts-1"
  voice: "alloy"

steps:
  - id: transcribe
    type: inference.transcribe
    input:
      model: "whisper-large-v3"
      audio: "${input.audio}"
  
  - id: chat
    type: inference.chat
    input:
      model: "${config.llm_model}"
      messages:
        - role: user
          content: "${steps.transcribe.text}"
    depends_on:
      - transcribe
  
  - id: synthesize
    type: inference.synthesize
    input:
      model: "${config.tts_model}"
      text: "${steps.chat.content}"
      voice: "${config.voice}"
    depends_on:
      - chat

output:
  text: "${steps.transcribe.text}"
  response: "${steps.chat.content}"
  audio: "${steps.synthesize.audio}"
